test.env <- matrix(unlist(test.mtn[1]),nrow=d,ncol=d)
str(test.env)
test.env <- matrix(test.mtn[1],nrow=d,ncol=d)
str(test.env)
test.env <- matrix(unlist(test.mtn[1]),nrow=d,ncol=d)
str(test.env)
test.div <- matrix(unlist(test.mtn[2]),nrow=d,ncol=d)
test.div
str(test.div)
# for each row of parameters, using a land generation thing
test.mtn <- matrix(unlist(mtn(10, 20, 0.2, 0.3, 5, 30, noise.x=40, noise.y=20, 100)[1]),nrow=d,nrow=d)
mtn(10, 20, 0.2, 0.3, 5, 30, noise.x=40, noise.y=20, 100)[1]
str(mtn(10, 20, 0.2, 0.3, 5, 30, noise.x=40, noise.y=20, 100)[1])
unlist(mtn(10, 20, 0.2, 0.3, 5, 30, noise.x=40, noise.y=20, 100)[1])
str(unlist(mtn(10, 20, 0.2, 0.3, 5, 30, noise.x=40, noise.y=20, 100)[1]))
# for each row of parameters, using a land generation thing
str(matrix(unlist(mtn(10, 20, 0.2, 0.3, 5, 30, noise.x=40, noise.y=20, 100)[1]),nrow=d,nrow=d))
# for each row of parameters, using a land generation thing
str(matrix(unlist(mtn(10, 20, 0.2, 0.3, 5, 30, noise.x=40, noise.y=20, 100)[1]),nrow=d,ncol=d))
sample.env <-
make.env <- function(land.gen, params){
env <- array(dim=c(d, d, nrow(params)))
for (i in seq_len(nrow(params))){
env[,,i] <- with(params, matrix(unlist(land.gen(Ax[i], Ay[i], Bx[i], By[i], slope[i], int[i], noise.x[i], noise.y[i], d[i])[1]), nrow=d[i], ncol=d[i]))
}
return(env)
}
test.mult.env <- make.env(wavy.grad, par.test)
sample.env <-
make.env <- function(land.gen, params){
env <- array(dim=c(d, d, nrow(params)))
for (i in seq_len(nrow(params))){
env[,,i] <-  matrix(unlist(with(params,land.gen(Ax[i], Ay[i], Bx[i], By[i], slope[i], int[i], noise.x[i], noise.y[i], d[i])[1])), nrow=d[i], ncol=d[i])
}
return(env)
}
test.mult.env <- make.env(wavy.grad, par.test)
# Mountainous
mtn <- function(Ax, Ay, Bx, By, slope, int, noise.x, noise.y, d){
env <- matrix(nrow=d, ncol=d)
div <- matrix(nrow=d, ncol=d)
for (i in seq_len(d)){
for (j in seq_len(d)){
env[i,j] <- ((Ax*cos(Bx*i)))+((Ay*cos(By*j))) + rnorm(1,mean=0,sd=noise.x)
div[i,j] <- slope*env[i,j] + int + rnorm(1,mean=0,sd=noise.y)
}
}
return(list(env, div))
}
# Wavy-linear gradient
wavy.grad <-function(Ax, Ay, Bx, By, slope, int, noise.x, noise.y, d){
env <- matrix(nrow=d, ncol=d)
div <- matrix(nrow=d, ncol=d)
for (i in seq_len(d)){
for (j in seq_len(d)){
env[i,j] <- (i+(Ax*cos(Bx*i)))+(j+(Ay*cos(By*j))) + rnorm(1,mean=0,sd=noise.x) #completely random variation in x but still deterministic of y
div[i,j] <- slope*env[i,j] + int + rnorm(1,mean=0,sd=noise.y) #completely random variation in y
}
}
return(list(env, div))
}
sample.env <-
make.env <- function(land.gen, params){
env <- array(dim=c(d, d, nrow(params)))
for (i in seq_len(nrow(params))){
env[,,i] <-  matrix(unlist(with(params,land.gen(Ax[i], Ay[i], Bx[i], By[i], slope[i], int[i], noise.x[i], noise.y[i], d[i])[1])), nrow=d[i], ncol=d[i])
}
return(env)
}
test.mult.env <- make.env(wavy.grad, par.test)
sample.env <-
make.env <- function(land.gen, params){
env <- array(dim=c(d, d, nrow(params)))
for (i in seq_len(nrow(params))){
env[,,i] <-  matrix(unlist(with(params,land.gen(Ax[i], Ay[i], Bx[i], By[i], slope[i], int[i], noise.x[i], noise.y[i], d[i])[1])), nrow=d, ncol=d)
}
return(env)
}
test.mult.env <- make.env(wavy.grad, par.test)
str(test.mult.env)
par.test
str(par.test)
make.div <- function(land.gen, params){
div <- array(dim=c(d, d, nrow(params)))
for (i in seq_len(nrow(params))){
div[,,i] <-  matrix(unlist(with(params,land.gen(Ax[i], Ay[i], Bx[i], By[i], slope[i], int[i], noise.x[i], noise.y[i], d[i])[2])), nrow=d, ncol=d)
}
return(div)
}
test.mult.div <- make.div(wavy.grad, par.test)
str(test.mult.div)
str(test.mult.env)
str(params)
str(test.params)
str(test.par)
str(par.test)
mtn.coefs <- c(2.2945, 2.2945+0.0004, 2.2945+0.0031, 2.2945+1.6629, 2.2945+0.4519, 2.2945+0.0482, 2.2945-3.9247, 2.2945+2.2516, 2.2945+0.1675, 2.2945+0.0433, 2.2945-0.1026)
# function for making each coefficient in logistic regression output a proportion
inv.logit <- function(x) exp(x)/(exp(x)+1)
inv.mtn.coefs <- inv.logit(mtn.coefs)
inv.mtn.coefs
mtn.coefs <- c(2.7027, 2.7027+0.0038, 2.7027-0.0140, 2.7027-0.2396, 2.7027+1.8768, 2.7027+0.0133, 2.7027-2.0104, 2.7027+0.5291, 2.7027+0.0365, 2.7027-0.1722, 2.7027-0.1722)
inv.mtn.coefs <- inv.logit(mtn.coefs)
inv.mtn.coefs
mtn.coefs <- c(2.4881, 2.4881-0.0007, 2.4881+0.0107, 2.4881-0.9970, 2.4881+0.1068, 2.4881-0.0147, 2.4881-3.6243, 2.4881+2.5654, 2.4881+0.0072, 2.4881-0.0647, 2.4881-0.2890)
inv.mtn.coefs <- inv.logit(mtn.coefs)
inv.mtn.coefs
mtn.coefs <- c(3.0177, 3.0177-0.0010, 3.0177-0.0127, 3.0177-1.0939, 3.0177-1.0747, 3.0177-0.0000, 3.0177-1.9250, 3.0177+0.9692, 3.0177+0.0331, 3.0177-0.2573, 3.0177-0.2573)
inv.mtn.coefs <- inv.logit(mtn.coefs)
inv.mtn.coefs
2916*2
###########
# HEADERS #
###########
library(pez)
library(vegan)
library(lme4)
library(visreg) # need to load this package to plot loaded .Rdata
?.ses.mpd
library(rstanarm)
citation(rstanarm)
?citation
citation("rstanarm")
citation("vegan")
dljfsldfs
dkfjdk'fjdsl'
?cortest
??cortest
?cor.test
# Environment (microclimate) at core RHF plots: soil temp (2017-18) & texture & terrain
# Elizabeth Simpson # 2019-07-29 | 2021-1-25
setwd("~/Documents/projects/functional_traits_rhf")
library(soiltexture)
library(lubridate)
library(readxl)
library(weathermetrics)
library(tidyverse)
#############
# FUNCTIONS #
#############
### To calculate soil texture ###
# Temperature correction function for hydrometer readings (+0.4 g/L for each degree above 20 C, - 0.4 g/L for each degree below 20 C)
temp.correct <- function(temp, hydro_rdg){
return(hydro_rdg + ((temp-20)*0.4))
}
# Soil texture calculations
texture <- function(soil, plot_ID, soil_wt, temp_calib, hydro_calib, temp_40, hydro_40, temp_120, hydro_120){
clean_soil <- matrix(nrow=nrow(soil), ncol=3)
for(i in 1:nrow(soil)){
B <- with(soil, temp.correct(temp_calib[i], hydro_calib[i]))
A_40 <- with(soil, temp.correct(temp_40[i], hydro_40[i]))
A_120 <- with(soil, temp.correct(temp_120[i], hydro_120[i]))
silt_clay <- ((A_40-B)/soil_wt[i])*100
clean_soil[i,1] <- 100-silt_clay #sand
clean_soil[i,2] <- ((A_120-B)/soil_wt[i])*100 #clay
clean_soil[i,3] <- 100 - (100-silt_clay) - (((A_120-B)/soil_wt[i])*100) #silt
}
colnames(clean_soil) <- c("SAND", "CLAY", "SILT")
output <- as.data.frame(cbind(plot_ID, clean_soil))
output$SAND <- as.numeric(output$SAND)
output$CLAY <- as.numeric(output$CLAY)
output$SILT <- as.numeric(output$SILT)
return(output)
}
# To format times in the temperature dataset - from code from MS
format.times <- function(x){
#x <- all_data[4,1]
hour <- as.character(hour(x))
minute <- as.character(minute(x))
if(nchar(hour) == 1) hour <- paste(0,hour,sep="")
if(nchar(minute) == 1) minute <- paste(0,minute,sep="")
return(paste(hour,minute,sep=":"))
}
#############
# LOAD DATA #
#############
### Topography - clean data from fractal paper
topo <- read.csv("./clean_data/clean_rhf_terrain_17-18.csv", as.is=TRUE)
### Soil texture
s.text <- read.csv("./raw_data/rhf_2018_soil_texture.csv", as.is=TRUE)
### Temperature - modified from code from MS
# Note, converted from .hobo file to .csv in HoboWare
temp.csvs <- list.files("./raw_data/17-18_temp/")[which(grepl("csv",list.files("raw_data/17-18_temp")))]
temp.csvs19 <- list.files("./raw_data/18-19_temp/")[which(grepl("csv",list.files("raw_Data/18-19_temp")))]
##############
# CLEAN DATA #
### Topography - remove extra first col and TPI from the dataframe
topo <- topo[,-c(1,10)]
topo$plot_id <- as.character(topo$plot_id)
### Soil texture
s.text <- na.omit(s.text)
text <- with(s.text, texture(s.text, plot_ID, soil_wt, temp_calib, hydro_calib, temp_40, hydro_40, temp_120, hydro_120))
text <- cbind(text, TT.points.in.classes(tri.data = text, class.sys = "USDA.TT", PiC.type="t"))
colnames(text) <- c("plot_id", "SAND", "CLAY", "SILT", "class")
### Temperature 2017-2018 - modified from code from MS
temp <- data.frame(datetime = rep(NA,10000*26),
temp = rep(NA,10000*26),
plot = rep(NA,10000*26))
top <- 1
for(i in temp.csvs){
file <- read.csv(paste("raw_data/17-18_temp/",i,sep=""), header=F, as.is=TRUE)[,c(2,3)]
colnames(file) <- c("datetime", "temp")
file <- file[-c(1,2),]
file[,1] <- file[,1]
file[,2] <- as.numeric(file[,2])
file[,3] <- as.integer(substr(i,1,4))
bottom <- top + nrow(file) - 1
temp[top:bottom,] <- file
top <- bottom + 1
}
temp <- temp[-which(is.na(temp$datetime)),]
temp[,1] <- mdy_hms(temp[,1], tz = "America/Denver") #if this throws an error, re install lubridate & restart R
# this next line takes a couple minutes to run
f_temp <- data.frame(plot = temp[,3],
datetime = temp[,1],
year = year(temp[,1]),
month = month(temp[,1]),
day = day(temp[,1]),
time = sapply(temp[,1], format.times),
temp = fahrenheit.to.celsius(temp[,2]))
f_temp <- na.omit(f_temp)
# Summarize date ranges to get time period completely covered by sensors - back to ES code from here on out
# Then, visually assess first and last date that covers the same range
date.range <- as.data.frame(f_temp %>%
group_by(plot) %>%
summarize(first.m=first(month),first.d=first(day), first.t=first(time), first.y=first(year),last.m=last(month),last.d=last(day), last.t=last(time), last.y=last(year)))
# subset data for 2017 and 2018 matched start and end.
f_temp <- subset(f_temp,
datetime >= as.POSIXct('2017-09-28 00:00:00',
tz = "MST") &
datetime <= as.POSIXct('2018-09-12 00:00:00',
tz = "MST"))
# Annual temp. summary stats - from 16 days shy of 12 months of data
# Calculate statistics by plot, could also summarize at the monthly level first and then at plot
yr.temp <- as.data.frame(group_by(f_temp, plot) %>%
summarize(mean(temp), sd(temp), max(temp), min(temp)))
colnames(yr.temp) <- c("plot_id", "mean", "sd", "max", "min")
# Mean Annual temperature - # look up how months are ordered in this! what does each number mean
# Only a half month of data for Sept. and Oct.
mat <- as.data.frame(group_by(f_temp, plot, month) %>%
summarize(mean(temp)))
colnames(mat) <- c("plot","month", "mean")
mat <- as.data.frame(group_by(mat, plot) %>%
summarize(mean(mean), sd(mean)))
colnames(mat) <- c("plot", "mean", "sd")
all.temp <- merge(yr.temp, mat, by.x="plot_id", by.y="plot")
colnames(all.temp) <-c("plot", "mean", "sd", "max", "min", "MAT", "MA_sd")
##########################################################
### Temperature 2018-2019 - modified from code from MS ###
temp19 <- data.frame(datetime = rep(NA,10000*24),
temp = rep(NA,10000*24),
plot = rep(NA,10000*24))
top <- 1
for(i in temp.csvs19){
file <- read.csv(paste("raw_data/18-19_temp/",i,sep=""), header=F, as.is=TRUE)[,c(2,3)]
colnames(file) <- c("datetime", "temp")
file <- file[-c(1,2),]
file[,1] <- file[,1]
file[,2] <- as.numeric(file[,2])
file[,3] <- as.integer(substr(i,1,4))
bottom <- top + nrow(file) - 1
temp19[top:bottom,] <- file
top <- bottom + 1
}
temp19 <- temp19[-which(is.na(temp19$datetime)),]
temp19[,1] <- mdy_hms(temp19[,1], tz = "America/Denver") #if this throws an error, re install lubridate & restart R
# this next line takes a couple minutes to run
f_temp19 <- data.frame(plot = temp19[,3],
datetime = temp19[,1],
year = year(temp19[,1]),
month = month(temp19[,1]),
day = day(temp19[,1]),
time = sapply(temp19[,1], format.times),
temp = fahrenheit.to.celsius(temp19[,2]))
f_temp19 <- na.omit(f_temp19)
# Summarize date ranges to get time period completely covered by sensors - back to ES code from here on out
# Then, visually assess first and last date that covers the same range
date.range19 <- as.data.frame(f_temp19 %>%
group_by(plot) %>%
summarize(first.m=first(month),first.d=first(day), first.t=first(time), first.y=first(year),last.m=last(month),last.d=last(day), last.t=last(time), last.y=last(year)))
date.range19
# subset data for 2018 and 2019 matched start and end
# NOTE: for this set of data, this cuts out quite a large number of plots, because there were many temperature sensors that power reset this year
f_temp19 <- subset(f_temp19,
datetime >= as.POSIXct('2018-10-03 00:00:00',
tz = "MST") &
datetime <= as.POSIXct('2019-06-26 00:00:00',
tz = "MST"))
f_temp19
# Annual temp. summary stats - from 16 days shy of 12 months of data
# Calculate statistics by plot, could also summarize at the monthly level first and then at plot
yr.temp19 <- as.data.frame(group_by(f_temp19, plot) %>%
summarize(mean(temp), sd(temp), max(temp), min(temp)))
colnames(yr.temp19) <- c("plot_id", "mean", "sd", "max", "min")
# Mean Annual temperature - # look up how months are ordered in this! what does each number mean
# Only a half month of data for Sept. and Oct.
mat19 <- as.data.frame(group_by(f_temp19, plot, month) %>%
summarize(mean(temp)))
colnames(mat19) <- c("plot","month", "mean")
mat19 <- as.data.frame(group_by(mat, plot) %>%
summarize(mean(mean), sd(mean)))
colnames(mat19) <- c("plot", "mean", "sd")
all.temp19 <- merge(yr.temp19, mat19, by.x="plot_id", by.y="plot")
colnames(all.temp19) <-c("plot", "mean", "sd", "max", "min", "MAT", "MA_sd")
# dataframe of terrain, texture, and temperature (2018-19) -
env.t19 <- merge(env, all.temp19, by.x="plot_id", by.y="plot")
# Environment (microclimate) at core RHF plots: soil temp (2017-18) & texture & terrain
# Elizabeth Simpson # 2019-07-29 | 2021-1-25
setwd("~/Documents/projects/functional_traits_rhf")
library(soiltexture)
library(lubridate)
library(readxl)
library(weathermetrics)
library(tidyverse)
#############
# FUNCTIONS #
#############
### To calculate soil texture ###
# Temperature correction function for hydrometer readings (+0.4 g/L for each degree above 20 C, - 0.4 g/L for each degree below 20 C)
temp.correct <- function(temp, hydro_rdg){
return(hydro_rdg + ((temp-20)*0.4))
}
# Soil texture calculations
texture <- function(soil, plot_ID, soil_wt, temp_calib, hydro_calib, temp_40, hydro_40, temp_120, hydro_120){
clean_soil <- matrix(nrow=nrow(soil), ncol=3)
for(i in 1:nrow(soil)){
B <- with(soil, temp.correct(temp_calib[i], hydro_calib[i]))
A_40 <- with(soil, temp.correct(temp_40[i], hydro_40[i]))
A_120 <- with(soil, temp.correct(temp_120[i], hydro_120[i]))
silt_clay <- ((A_40-B)/soil_wt[i])*100
clean_soil[i,1] <- 100-silt_clay #sand
clean_soil[i,2] <- ((A_120-B)/soil_wt[i])*100 #clay
clean_soil[i,3] <- 100 - (100-silt_clay) - (((A_120-B)/soil_wt[i])*100) #silt
}
colnames(clean_soil) <- c("SAND", "CLAY", "SILT")
output <- as.data.frame(cbind(plot_ID, clean_soil))
output$SAND <- as.numeric(output$SAND)
output$CLAY <- as.numeric(output$CLAY)
output$SILT <- as.numeric(output$SILT)
return(output)
}
# To format times in the temperature dataset - from code from MS
format.times <- function(x){
#x <- all_data[4,1]
hour <- as.character(hour(x))
minute <- as.character(minute(x))
if(nchar(hour) == 1) hour <- paste(0,hour,sep="")
if(nchar(minute) == 1) minute <- paste(0,minute,sep="")
return(paste(hour,minute,sep=":"))
}
#############
# LOAD DATA #
#############
### Topography - clean data from fractal paper
topo <- read.csv("./clean_data/clean_rhf_terrain_17-18.csv", as.is=TRUE)
### Soil texture
s.text <- read.csv("./raw_data/rhf_2018_soil_texture.csv", as.is=TRUE)
### Temperature - modified from code from MS
# Note, converted from .hobo file to .csv in HoboWare
temp.csvs <- list.files("./raw_data/17-18_temp/")[which(grepl("csv",list.files("raw_data/17-18_temp")))]
temp.csvs19 <- list.files("./raw_data/18-19_temp/")[which(grepl("csv",list.files("raw_Data/18-19_temp")))]
##############
# CLEAN DATA #
### Topography - remove extra first col and TPI from the dataframe
topo <- topo[,-c(1,10)]
topo$plot_id <- as.character(topo$plot_id)
### Soil texture
s.text <- na.omit(s.text)
text <- with(s.text, texture(s.text, plot_ID, soil_wt, temp_calib, hydro_calib, temp_40, hydro_40, temp_120, hydro_120))
text <- cbind(text, TT.points.in.classes(tri.data = text, class.sys = "USDA.TT", PiC.type="t"))
colnames(text) <- c("plot_id", "SAND", "CLAY", "SILT", "class")
### Temperature 2017-2018 - modified from code from MS
temp <- data.frame(datetime = rep(NA,10000*26),
temp = rep(NA,10000*26),
plot = rep(NA,10000*26))
top <- 1
for(i in temp.csvs){
file <- read.csv(paste("raw_data/17-18_temp/",i,sep=""), header=F, as.is=TRUE)[,c(2,3)]
colnames(file) <- c("datetime", "temp")
file <- file[-c(1,2),]
file[,1] <- file[,1]
file[,2] <- as.numeric(file[,2])
file[,3] <- as.integer(substr(i,1,4))
bottom <- top + nrow(file) - 1
temp[top:bottom,] <- file
top <- bottom + 1
}
temp <- temp[-which(is.na(temp$datetime)),]
temp[,1] <- mdy_hms(temp[,1], tz = "America/Denver") #if this throws an error, re install lubridate & restart R
# this next line takes a couple minutes to run
f_temp <- data.frame(plot = temp[,3],
datetime = temp[,1],
year = year(temp[,1]),
month = month(temp[,1]),
day = day(temp[,1]),
time = sapply(temp[,1], format.times),
temp = fahrenheit.to.celsius(temp[,2]))
f_temp <- na.omit(f_temp)
# Summarize date ranges to get time period completely covered by sensors - back to ES code from here on out
# Then, visually assess first and last date that covers the same range
date.range <- as.data.frame(f_temp %>%
group_by(plot) %>%
summarize(first.m=first(month),first.d=first(day), first.t=first(time), first.y=first(year),last.m=last(month),last.d=last(day), last.t=last(time), last.y=last(year)))
# subset data for 2017 and 2018 matched start and end.
f_temp <- subset(f_temp,
datetime >= as.POSIXct('2017-09-28 00:00:00',
tz = "MST") &
datetime <= as.POSIXct('2018-09-12 00:00:00',
tz = "MST"))
# Annual temp. summary stats - from 16 days shy of 12 months of data
# Calculate statistics by plot, could also summarize at the monthly level first and then at plot
yr.temp <- as.data.frame(group_by(f_temp, plot) %>%
summarize(mean(temp), sd(temp), max(temp), min(temp)))
colnames(yr.temp) <- c("plot_id", "mean", "sd", "max", "min")
# Mean Annual temperature - # look up how months are ordered in this! what does each number mean
# Only a half month of data for Sept. and Oct.
mat <- as.data.frame(group_by(f_temp, plot, month) %>%
summarize(mean(temp)))
colnames(mat) <- c("plot","month", "mean")
mat <- as.data.frame(group_by(mat, plot) %>%
summarize(mean(mean), sd(mean)))
colnames(mat) <- c("plot", "mean", "sd")
all.temp <- merge(yr.temp, mat, by.x="plot_id", by.y="plot")
colnames(all.temp) <-c("plot", "mean", "sd", "max", "min", "MAT", "MA_sd")
##########################################################
### Temperature 2018-2019 - modified from code from MS ###
temp19 <- data.frame(datetime = rep(NA,10000*24),
temp = rep(NA,10000*24),
plot = rep(NA,10000*24))
top <- 1
for(i in temp.csvs19){
file <- read.csv(paste("raw_data/18-19_temp/",i,sep=""), header=F, as.is=TRUE)[,c(2,3)]
colnames(file) <- c("datetime", "temp")
file <- file[-c(1,2),]
file[,1] <- file[,1]
file[,2] <- as.numeric(file[,2])
file[,3] <- as.integer(substr(i,1,4))
bottom <- top + nrow(file) - 1
temp19[top:bottom,] <- file
top <- bottom + 1
}
temp19 <- temp19[-which(is.na(temp19$datetime)),]
temp19[,1] <- mdy_hms(temp19[,1], tz = "America/Denver") #if this throws an error, re install lubridate & restart R
# this next line takes a couple minutes to run
f_temp19 <- data.frame(plot = temp19[,3],
datetime = temp19[,1],
year = year(temp19[,1]),
month = month(temp19[,1]),
day = day(temp19[,1]),
time = sapply(temp19[,1], format.times),
temp = fahrenheit.to.celsius(temp19[,2]))
f_temp19 <- na.omit(f_temp19)
# Summarize date ranges to get time period completely covered by sensors - back to ES code from here on out
# Then, visually assess first and last date that covers the same range
date.range19 <- as.data.frame(f_temp19 %>%
group_by(plot) %>%
summarize(first.m=first(month),first.d=first(day), first.t=first(time), first.y=first(year),last.m=last(month),last.d=last(day), last.t=last(time), last.y=last(year)))
# subset data for 2018 and 2019 matched start and end
# NOTE: for this set of data, this cuts out quite a large number of plots, because there were many temperature sensors that power reset this year
f_temp19 <- subset(f_temp19,
datetime >= as.POSIXct('2018-10-03 00:00:00',
tz = "MST") &
datetime <= as.POSIXct('2019-06-26 00:00:00',
tz = "MST"))
# Annual temp. summary stats - from 16 days shy of 12 months of data
# Calculate statistics by plot, could also summarize at the monthly level first and then at plot
yr.temp19 <- as.data.frame(group_by(f_temp19, plot) %>%
summarize(mean(temp), sd(temp), max(temp), min(temp)))
colnames(yr.temp19) <- c("plot_id", "mean", "sd", "max", "min")
# Mean Annual temperature - # look up how months are ordered in this! what does each number mean
# Only a half month of data for Sept. and Oct.
mat19 <- as.data.frame(group_by(f_temp19, plot, month) %>%
summarize(mean(temp)))
colnames(mat19) <- c("plot","month", "mean")
mat19 <- as.data.frame(group_by(mat, plot) %>%
summarize(mean(mean), sd(mean)))
colnames(mat19) <- c("plot", "mean", "sd")
all.temp19 <- merge(yr.temp19, mat19, by.x="plot_id", by.y="plot")
colnames(all.temp19) <-c("plot", "mean", "sd", "max", "min", "MAT", "MA_sd")
###########################
# Put all env data together
# Make a csv for texture, + a dataframe of topo and texture + a dataframe of everything (what you'll use for the analysis)
# clean 2018 soil texture
write.csv(text, "./clean_data/soil-texture-18.csv")
# dataframe of terrain and texture to use in analysis - 76 plots
env <- merge(topo, text, by.x="plot_id", by.y="plot_id")
write.csv(env, "./clean_data/texture-terrain-18.csv")
# dataframe of terrain, texture, and temperature (2017-18) - 25 plots
env.t <- merge(env, all.temp, by.x="plot_id", by.y="plot")
write.csv(env.t, "./clean_data/temp-texture-terrain-18.csv")
# dataframe of terrain, texture, and temperature (2018-19) -
env.t19 <- merge(env, all.temp19, by.x="plot_id", by.y="plot")
env.t19
all.temp19
mat19
# Mean Annual temperature - # look up how months are ordered in this! what does each number mean
# Only a half month of data for Sept. and Oct.
mat19 <- as.data.frame(group_by(f_temp19, plot, month) %>%
summarize(mean(temp)))
mat19
colnames(mat19) <- c("plot","month", "mean")
mat19 <- as.data.frame(group_by(mat, plot) %>%
summarize(mean(mean), sd(mean)))
colnames(mat19) <- c("plot", "mean", "sd")
mat19
all.temp19 <- merge(yr.temp19, mat19, by.x="plot_id", by.y="plot")
colnames(all.temp19) <-c("plot", "mean", "sd", "max", "min", "MAT", "MA_sd")
all.temp19
# dataframe of terrain, texture, and temperature (2018-19) -
env.t19 <- merge(env, all.temp19[,1:5], by.x="plot_id", by.y="plot")
write.csv(env.t19, "./clean_data/temp-texture-terrain-19.csv")
env.t19
